{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Dot, Reshape, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LambdaCallback\n",
    "import keras.backend as K\n",
    "from keras.initializers import Constant, RandomUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where do Keras embedding values come from?\n",
    "In this notebook I demonstrate what happens under the hood in Keras' Embedding Layer by examining a simple yet real-world model that generates book embeddings for a book recommendation system. The original article is described [here](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526). Additionally, the author has also made his source code public [here](https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb) and I'll draw heavily from it. I'll first give a high level overview of his approach but you should really go check out the source to get a fuller understanding! Then we'll peel back the veil and see how we can predict with certainty exactly how the embeddings are made. \n",
    "\n",
    "## Overview\n",
    "The goal of the original project was to represent all the books on Wikipedia as vectors to create a book recommendation system. The supervised learning task requires the model to predict whether a link to a Wikipedia article appears in the article for a book. The hypothesis is that books with Wikipedia articles containing the same link to another Wikipedia article will themselves be similar. For example, the Wiki article [War and Peace](https://en.wikipedia.org/wiki/War_and_Peace) contains a link to [Anna Karenina](https://en.wikipedia.org/wiki/Anna_Karenina), itself a book. Thus these two books are likely related (indeed, they are written by the same author, [Leo Tolstoy](https://en.wikipedia.org/wiki/Leo_Tolstoy)). \n",
    "\n",
    "The model learns embedding vectors for each book such that similar books are closer to each other in the embedding space. Training the model consists of feeding it (book, link) pairs with a mix of positive (true) and negative (false) examples. For instance, a positive training example would be (War and Peace, Anna Karenina), while a negative one might be (War and Peace, kittens).  This practice of *negative sampling* is very common for training embeddings; the technique is also used in the word2vec and fastText word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book Embedding Model\n",
    "The book embedding model is created with Keras and consists of 5 key components.  \n",
    "\n",
    "1.  __Input Layer__: Each book is assigned a unique integer (*War and Peace* might be Book 3). Links are also assigned unique integers (*kittens* might be Link 0). A pair of integers representing a book and a link, (3, 0), is passed to the model. \n",
    "2.  __Embedding Layer__: These integers act as look-up values and select the corresponding row in the associated embedding matrix. 3 selects the third row vector in the book embedding matrix, while 0 selects the zeroth row in the link embedding matrix. \n",
    "<img src=\"book_embedding_matrix.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "3. __Dot Product__: Both the book embedding vector and link embedding vector have the same dimension (the embedding dimension, which must be chosen beforehand). Our loss function requires a scalar so we take a dot product of the two vectors. \n",
    "4. __Loss Function__: The loss function is the Mean Squared Error which takes in the true label (in this case, -1 because the *kittens* link does NOT appear in the *War and Peace* Wiki article), and the value of the Dot Product produced in Step 3. \n",
    "5. __Optimization__: In this example we'll use the simplest algorithm, vanilla Stochastic Gradient Descent.\n",
    "\n",
    "<img src=\"book_embedding_graph.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Okay, so tell me where all these embedding numbers come from (aka, math time!)\n",
    "The Keras Embedding Layer acts almost like a regular Dense Layer with two key differences. First, the Embedding Layer takes integers as input instead of real values. Secondly, the Embedding Layer typically does not have an activation function applied to it. But just like the weights in a Dense Layer, the embedding *weights* are updated during training through [backpropagation](https://en.wikipedia.org/wiki/Backpropagation). \n",
    "\n",
    "The weight update rules goes as\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{\\text{book}_j}^{\\text{(new)}} = w_{\\text{book}_j}^{\\text{(old)}} - \\eta\\delta_{\\text{book}_j}\n",
    "\\end{equation*}\n",
    "\n",
    "where each new element, *j*, of the book embedding vector can be computed by subtracting from the original value the quantity $\\eta*\\delta$. Here, $\\eta$ is the learning rate and $\\delta$ is the derivative of the cost function with respect to the weights of the book embedding. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\delta_{\\text{book}_j} = \\frac{\\partial J}{\\partial w_{\\text{book}_j}} \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "As discussed above, we know the cost function is the Mean Squared Error: \n",
    "\n",
    "\\begin{equation*}\n",
    "J = \\frac{1}{n}\\sum (Y_i - \\hat Y_i)^2\n",
    "\\end{equation*}\n",
    "\n",
    "In the example code that follows we'll be using a batch size of 1 so our cost function reduces to \n",
    "\n",
    "\\begin{equation*}\n",
    "J = (Y_i - \\hat Y_i)^2\n",
    "\\end{equation*}\n",
    "\n",
    "where $Y_i$ is the true label and $\\hat Y_i$ is given by the dot product between the book embedding vector and link embedding vector:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat Y_i = \\mathbf{w}_{\\text{book}} \\cdot \\mathbf{w}_{\\text{link}} = \\sum_{j=1}^d w_{\\text{book}_j} w_{\\text{link}_j}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Now we can use the chain rule to get\n",
    "\\begin{align}\n",
    "\\delta_{\\text{book}_j} & = \\frac{\\partial J}{\\partial w_{\\text{book}_j}}\\\\ \n",
    "                       & = \\frac{\\partial J}{\\partial \\hat Y_i} \\frac{\\partial \\hat Y_i}{\\partial w_{\\text{book}_j}}\\\\\n",
    "                       & = 2(Y_i - \\hat Y_i)w_{\\text{link}_j}\n",
    "\\end{align}\n",
    "\n",
    "Therefore, our final update rule is\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{\\text{book}_j}^{\\text{(new)}} = w_{\\text{book}_j}^{\\text{(old)}} - 2\\eta(Y_i - \\hat Y_i)w_{\\text{link}_j}\n",
    "\\end{equation*}\n",
    "\n",
    "We can perform the same analysis for the link embedding matrix and we'd get\n",
    "\n",
    "\\begin{equation*}\n",
    "w_{\\text{link}_j}^{\\text{(new)}} = w_{\\text{link}_j}^{\\text{(old)}} - 2\\eta(Y_i - \\hat Y_i)w_{\\text{book}_j}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "So there we have it! If we did our math right, we should be able to use these to compute the new embedding values during training! Let's check our work with a real(ish) example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Let's see this in action!\n",
    "Time to apply what we've learned to real code! I'm going to simplify the problem even further by pretending there are only 5 books on Wikipedia and only 3 links among them (what a sad universe). The original application had 50 embedding dimensions but I'm only going to use 5 so that the matrices we print out have a chance at being readable. Finally, I'm only going to feed in positive training examples, but the option exists for you to add in negative ones. I'll create a toy training set and we'll feed in one instance at a time. We'll print out the resulting embedding matrix after each batch and see how the embedding matrix updates. Then we'll code up the update rules we derived above and compare our results with Keras! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a fake training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my sad universe there are only 5 books in the world and 3 links on the internet\n",
    "num_books = 5\n",
    "num_links = 3\n",
    "\n",
    "# Each book is represented with integers 0 through 4. \n",
    "# Each link is represented with integers 0 through 2.\n",
    "# The pair (0, 1) thus denotes (Book 0, Link 1)\n",
    "\n",
    "# Now to create the training set itself: (book, link) pairs\n",
    "# (I just made up these pairs but they'll do for our purpose)\n",
    "pairs = [(0, 1), (1, 0), (2, 0), (3, 2), (4, 1), (0, 0), (1, 2), (2, 2)]\n",
    "\n",
    "pairs_set = set(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a batch generator\n",
    "The *pairs* list above represents our entire (fake) training set. Each tuple corresponds to a (book, link) pair. We will train a Keras model with this data but we need to control how these examples are fed into the model so that we can understand the output. For that, we need a generator which I've based off the one in the original code [here](https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb). This generator essentially loops through the training set in order, passing the requested number of training examples to the model. It also prints out which (book, link) pairs were sent, making it easy for us to keep track of the training process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_in_order(pairs, n_positive=50, negative_ratio=1.0):\n",
    "    \"\"\"  \n",
    "    This generator creates batches that are drawn in order from the input, pairs, which contains positive examples.\n",
    "    Negative examples can also be generated according to the negative_ratio. \n",
    "    Once the generator has reached the end of the input, it loops back to the beginning. \n",
    "    \n",
    "    This is not how you should train a NN in practice! But it is quite handy for examining the \n",
    "    output of the Keras model since we know exactly which examples it will see and in what order. \n",
    "    \n",
    "    pairs          = list of (book integer, link integer) positive instances\n",
    "    n_positive     = number of positive instances to include in the batch\n",
    "    negative_ratio = ratio of positive to negative samples to include in the batch\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    print(\"batch_size:\", batch_size)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "\n",
    "    neg_label = -1\n",
    "\n",
    "    j = 0\n",
    "    i = 0\n",
    "    while True:\n",
    "        book_id, link_id = pairs[j]\n",
    "        batch[i, :] = (book_id, link_id, 1)\n",
    "        j += 1\n",
    "        i += 1\n",
    "\n",
    "        # If we reach the end of the number of \"fixed\" values for this array, send the batch\n",
    "        if i >= n_positive:\n",
    "            # But first, fill up the rest of the space with randomly drawn values\n",
    "            for k in range(i, batch_size):\n",
    "                # random selection\n",
    "                random_book = random.randrange(books)\n",
    "                random_link = random.randrange(links)\n",
    "\n",
    "                # Check to make sure this is not a positive example\n",
    "                if (random_book, random_link) not in pairs_set:\n",
    "                    # Add to batch and increment index\n",
    "                    batch[k, :] = (random_book, random_link, neg_label)\n",
    "\n",
    "            print('\\nbooks:', batch[:, 0], 'links:', batch[:, 1], 'labels:', batch[:, 2])\n",
    "            yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]\n",
    "            i = 0\n",
    "\n",
    "        # If we reach the end of the original array, reset indices\n",
    "        if j == len(pairs):\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "This model is pulled directly from the original [source code](https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb) so go check it out! I've made some minor changes that streamline or simplify the model for the purpose of this tutorial and these are noted in the comments. One major addition is the use of Keras [Callbacks](https://keras.io/callbacks/), functions that can be applied at given stages of the training procedure. These allow us to print out the value of the embedding matrices after each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_embedding_model(embedding_size=5, book_callback=True, link_callback=False):\n",
    "    \"\"\"Model to embed books and wikilinks using the functional API.\n",
    "       Trained to discern if a link is present in a article\"\"\"\n",
    "\n",
    "    # Both inputs are 1-dimensional\n",
    "    book = Input(name='book', shape=[1])\n",
    "    link = Input(name='link', shape=[1])\n",
    "\n",
    "    # Embedding the book (shape will be (None, 1, embedding_size))\n",
    "    # Initial embedding values are seeded for reproducibility\n",
    "    book_embedding = Embedding(name = 'book_embedding',\n",
    "                               embeddings_initializer = RandomUniform(minval=-0.05, maxval=0.05, seed=42),\n",
    "                               input_dim = num_books,\n",
    "                               output_dim = embedding_size,\n",
    "                               input_length=1)(book)\n",
    "\n",
    "    # Embedding the link (shape will be (None, 1, embedding_size))\n",
    "    # Initial embedding values are seeded for reproducibility\n",
    "    link_embedding = Embedding(name = 'link_embedding',\n",
    "                               embeddings_initializer = RandomUniform(minval=-0.05, maxval=0.05, seed=43),\n",
    "                               input_dim = num_links,\n",
    "                               output_dim = embedding_size)(link)\n",
    "\n",
    "    # Merge the layers with a dot product along the second axis (shape will be (None, 1, 1))\n",
    "    # Turned 'norm' off to simplify backprop math\n",
    "    merged = Dot(name='dot_product', normalize=False, axes=2)([book_embedding, link_embedding])\n",
    "\n",
    "    # Reshape to be a single number (shape will be (None, 1))\n",
    "    merged = Reshape(target_shape=[1])(merged)\n",
    "\n",
    "    model = Model(inputs=[book, link], outputs=merged)\n",
    "\n",
    "    # Adding Callbacks to print out embedding matrices during training\n",
    "    callbacks = []\n",
    "    if book_callback:\n",
    "        book_callback = LambdaCallback(\n",
    "                on_batch_end=lambda batch, logs: print(model.get_layer('book_embedding').get_weights()[0]))\n",
    "        callbacks.append(book_callback)\n",
    "    \n",
    "    if link_callback:\n",
    "        link_callback = LambdaCallback(\n",
    "                on_batch_end=lambda batch, logs: print(model.get_layer('link_embedding').get_weights()[0]))\n",
    "        callbacks.append(link_callback)\n",
    "        \n",
    "    # Switching to SGD optimizer for simplification\n",
    "    model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "    # Print out the initial embedding weights -- before any training has occured\n",
    "    book_weights = model.get_layer(\"book_embedding\").get_weights()[0]\n",
    "    link_weights = model.get_layer(\"link_embedding\").get_weights()[0]\n",
    "    print(\"before-training book embedding matrix:\\n\", book_weights)\n",
    "    print()\n",
    "    print(\"before-training link embedding matrix:\\n\",link_weights)\n",
    "    print()\n",
    "\n",
    "    return model, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model\n",
    "I've added code above such that, upon model initialization, the book and link embedding matrices are printed out before training even occurs! I've seeded the random mechanism so that they are initialized to the same set of values every time (yay for reproducibility!). This is our embedding starting point and we'll use these to compare between Keras' update mechanism and our own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before-training book embedding matrix:\n",
      " [[ 0.04522714  0.01774077  0.02953183  0.02557817 -0.00240444]\n",
      " [ 0.01310148 -0.03139796 -0.03856922 -0.01637782  0.02233351]\n",
      " [-0.02808004  0.03573376  0.03239204  0.00954127 -0.04970373]\n",
      " [-0.02527453  0.00060741 -0.01384113 -0.04551616  0.04721661]\n",
      " [ 0.03283885 -0.0085416   0.0101666  -0.01604132  0.00731027]]\n",
      "\n",
      "before-training link embedding matrix:\n",
      " [[-0.02405849 -0.04089488 -0.043049   -0.04481744 -0.00090784]\n",
      " [-0.04059631 -0.00314282 -0.02813883 -0.00767363 -0.01062437]\n",
      " [ 0.00983997  0.0189384  -0.03238429 -0.01509678  0.04887613]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the embedding dimension to be something that we can manageably print out\n",
    "embedding_size = 5\n",
    "\n",
    "# Batch size is one with no negative samples for illustrative purposes\n",
    "n_positive = 1\n",
    "\n",
    "# Create a training data generator\n",
    "gen = generate_batch_in_order(pairs, n_positive, negative_ratio = 0)\n",
    "\n",
    "# Instantiate model and show parameters\n",
    "model, callbacks = book_embedding_model(embedding_size=embedding_size, book_callback=True, link_callback=False)\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the book embedding matrix is randomly initialized and has shape (5,5) where each row corresponds to one of the 5 books in our universe and the columns represent the number of embedding dimensions. Similarly, the link embedding matrix has shape (3,5) because the embedding dimension is the same but there are only 3 lonely links in this sad universe. \n",
    "\n",
    "## Train the Model\n",
    "Now let's see what happens when we train the model! The generator will pass ONE training example to the model at a time and the book embedding matrix will print out after each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "batch_size: 1\n",
      "\n",
      "books: [0.] links: [1.] labels: [1.]\n",
      "[[ 0.04441287  0.01767774  0.02896742  0.02542426 -0.00261754]\n",
      " [ 0.01310148 -0.03139796 -0.03856922 -0.01637782  0.02233351]\n",
      " [-0.02808004  0.03573376  0.03239204  0.00954127 -0.04970373]\n",
      " [-0.02527453  0.00060741 -0.01384113 -0.04551616  0.04721661]\n",
      " [ 0.03283885 -0.0085416   0.0101666  -0.01604132  0.00731027]]\n",
      "\n",
      "books: [1.] links: [0.] labels: [1.]\n",
      "[[ 0.04441287  0.01767774  0.02896742  0.02542426 -0.00261754]\n",
      " [ 0.01262192 -0.03221313 -0.03942733 -0.01727117  0.02231541]\n",
      " [-0.02808004  0.03573376  0.03239204  0.00954127 -0.04970373]\n",
      " [-0.02527453  0.00060741 -0.01384113 -0.04551616  0.04721661]\n",
      " [ 0.03283885 -0.0085416   0.0101666  -0.01604132  0.00731027]]\n",
      "\n",
      "books: [2.] links: [0.] labels: [1.]\n",
      "[[ 0.04441287  0.01767774  0.02896742  0.02542426 -0.00261754]\n",
      " [ 0.01262192 -0.03221313 -0.03942733 -0.01727117  0.02231541]\n",
      " [-0.02855724  0.03490115  0.03151336  0.00863601 -0.049713  ]\n",
      " [-0.02527453  0.00060741 -0.01384113 -0.04551616  0.04721661]\n",
      " [ 0.03283885 -0.0085416   0.0101666  -0.01604132  0.00731027]]\n",
      "\n",
      "books: [3.] links: [2.] labels: [1.]\n",
      "[[ 0.04441287  0.01767774  0.02896742  0.02542426 -0.00261754]\n",
      " [ 0.01262192 -0.03221313 -0.03942733 -0.01727117  0.02231541]\n",
      " [-0.02855724  0.03490115  0.03151336  0.00863601 -0.049713  ]\n",
      " [-0.02507836  0.00098496 -0.01448674 -0.04581713  0.048191  ]\n",
      " [ 0.03283885 -0.0085416   0.0101666  -0.01604132  0.00731027]]\n",
      "\n",
      "books: [4.] links: [1.] labels: [1.]\n",
      "[[ 0.04441287  0.01767774  0.02896742  0.02542426 -0.00261754]\n",
      " [ 0.01262192 -0.03221313 -0.03942733 -0.01727117  0.02231541]\n",
      " [-0.02855724  0.03490115  0.03151336  0.00863601 -0.049713  ]\n",
      " [-0.02507836  0.00098496 -0.01448674 -0.04581713  0.048191  ]\n",
      " [ 0.03204386 -0.00859742  0.00961483 -0.01618475  0.00709649]]\n",
      "\n",
      "books: [0.] links: [0.] labels: [1.]\n",
      "[[ 0.04392362  0.01685823  0.02810043  0.02452144 -0.00264685]\n",
      " [ 0.01262192 -0.03221313 -0.03942733 -0.01727117  0.02231541]\n",
      " [-0.02855724  0.03490115  0.03151336  0.00863601 -0.049713  ]\n",
      " [-0.02507836  0.00098496 -0.01448674 -0.04581713  0.048191  ]\n",
      " [ 0.03204386 -0.00859742  0.00961483 -0.01618475  0.00709649]]\n",
      "\n",
      "books: [1.] links: [2.] labels: [1.]\n",
      "[[ 0.04392362  0.01685823  0.02810043  0.02452144 -0.00264685]\n",
      " [ 0.01280824 -0.03183494 -0.0400791  -0.01759056  0.02330958]\n",
      " [-0.02855724  0.03490115  0.03151336  0.00863601 -0.049713  ]\n",
      " [-0.02507836  0.00098496 -0.01448674 -0.04581713  0.048191  ]\n",
      " [ 0.03204386 -0.00859742  0.00961483 -0.01618475  0.00709649]]\n",
      "\n",
      "books: [2.] links: [2.] labels: [1.]\n",
      "[[ 0.04392362  0.01685823  0.02810043  0.02452144 -0.00264685]\n",
      " [ 0.01280824 -0.03183494 -0.0400791  -0.01759056  0.02330958]\n",
      " [-0.02836484  0.03526853  0.0308422   0.00830794 -0.0487044 ]\n",
      " [-0.02507836  0.00098496 -0.01448674 -0.04581713  0.048191  ]\n",
      " [ 0.03204386 -0.00859742  0.00961483 -0.01618475  0.00709649]]\n",
      " - 0s - loss: 1.0015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x136388f28>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(gen, epochs = 1,\n",
    "                    steps_per_epoch = len(pairs_set) // n_positive,\n",
    "                    verbose = 2,\n",
    "                    callbacks = callbacks,\n",
    "                    workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the training output\n",
    "Compare the before-training book embedding matrix to how it looks after processing the first training example: (0, 1). \n",
    "\n",
    "\\begin{align}\n",
    "\\text{before-training} & = \n",
    "    \\begin{bmatrix}\n",
    "        0.04522714 & 0.01774077 & 0.02953183 & 0.02557817 & -0.00240444\\\\\n",
    "        0.01310148 & -0.03139796 & -0.03856922 & -0.01637782 & 0.02233351\\\\\n",
    "        -0.02808004 & 0.03573376 & 0.03239204 & 0.00954127 & -0.04970373\\\\\n",
    "        -0.02527453 & 0.00060741 & -0.01384113 & -0.04551616 & 0.04721661\\\\\n",
    "        0.03283885 & -0.0085416 &  0.0101666 & -0.01604132 & 0.00731027\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "\\text{after-one-example} & = \n",
    "    \\begin{bmatrix}\n",
    "        \\mathbf{0.04441287} & \\mathbf{0.01767774} & \\mathbf{0.02896742} & \\mathbf{0.02542426} & \\mathbf{-0.00261754}\\\\\n",
    "        0.01310148 & -0.03139796 & -0.03856922 & -0.01637782 & 0.02233351 \\\\\n",
    "        -0.02808004 & 0.03573376 & 0.03239204 & 0.00954127 & -0.04970373 \\\\\n",
    "        -0.02527453 & 0.00060741 & -0.01384113 & -0.04551616 & 0.04721661\\\\\n",
    "        0.03283885 & -0.0085416 &  0.0101666 & -0.01604132 & 0.00731027\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "If you look closely you'll see that only the 0th vector has new values in it! All the other vectors are the same as before! This is exactly what we expected from our analysis above. If you follow the training output you'll see that each row of the book embedding matrix only receives updated values after the model processes a training example containing the corresponding book integer. \n",
    "\n",
    "Now that we know which values are updating, let's verify that we understand exactly how we get the new values. For this, we'll create our own update rules according to the backpropagation math we did earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our own update rules\n",
    "The math we did earlier comes in handy now! We'll use it to create functions that compute the new embedding matrix. We'll need to pass both the book and link embeddings to both functions because they are dependent on each other (thanks to the dot product), as well as the learning rate, $\\eta$, and the true label, $Y_i$. \n",
    "\n",
    "\\begin{align}\n",
    "w_{\\text{book}_j}^{\\text{(new)}} & = w_{\\text{book}_j}^{\\text{(old)}} - 2 \\eta (Y_i - \\hat Y_i)w_{\\text{link}_j}\\\\\n",
    "\\\\\n",
    "w_{\\text{link}_j}^{\\text{(new)}} & = w_{\\text{link}_j}^{\\text{(old)}} - 2\\eta(Y_i - \\hat Y_i)w_{\\text{book}_j}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for updating embedding weights for the Book Embedding model according to the rules of backpropagation.\n",
    "\n",
    "def update_book_embedding(book_embedding, link_embedding, label, learning_rate):\n",
    "    # Compute the dot product, y_hat\n",
    "    y_hat = np.dot(book_embedding, link_embedding)\n",
    "    \n",
    "    # Compute the updated weights according to:\n",
    "    # w_book_new = w_book_old - learning_rate * delta_book\n",
    "    #            = w_book_old - learning_rate * (2 * (label - y_hat) * link_embedding)\n",
    "    \n",
    "    book_embedding_new = book_embedding - learning_rate * 2 * (y_hat - label) * link_embedding\n",
    "    \n",
    "    return book_embedding_new\n",
    "\n",
    "\n",
    "def update_link_embedding(book_embedding, link_embedding, label, learning_rate):\n",
    "    # Compute the dot product\n",
    "    y_hat = np.dot(book_embedding, link_embedding)\n",
    "    \n",
    "    # Compute the updated weights according to\n",
    "    # w_link_new = w_link_old - learning_rate * delta_link\n",
    "    #            = w_link_old - learning_rate * (2 * (label - y_hat) * book_embedding)\n",
    "    \n",
    "    link_embedding_new = link_embedding - learning_rate * 2 * (y_hat - label) * book_embedding\n",
    "    \n",
    "    return link_embedding_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our update rules\n",
    "We'll start with the same before-training embedding matrices that the Keras model started with. Then we'll simply loop through the training set (pairs) and apply our update rules. If we did the math right, the resulting output should match what Keras output above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The initial book and link embedding matrices before any training \n",
    "# (Go ahead -- check that these are the same as above)\n",
    "\n",
    "book_embedding = np.array([[ 0.04522714,  0.01774077,  0.02953183,  0.02557817, -0.00240444],\n",
    "                           [ 0.01310148, -0.03139796, -0.03856922, -0.01637782,  0.02233351],\n",
    "                           [-0.02808004,  0.03573376,  0.03239204,  0.00954127, -0.04970373],\n",
    "                           [-0.02527453,  0.00060741, -0.01384113, -0.04551616,  0.04721661],\n",
    "                           [ 0.03283885, -0.0085416 ,  0.0101666 , -0.01604132,  0.00731027]])\n",
    "\n",
    "link_embedding = np.array([[-0.02405849, -0.04089488, -0.043049,   -0.04481744, -0.00090784],\n",
    "                           [-0.04059631, -0.00314282, -0.02813883, -0.00767363, -0.01062437],\n",
    "                           [ 0.00983997,  0.0189384,  -0.03238429, -0.01509678,  0.04887613]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book: 0 link: 1\n",
      "New book embedding:\n",
      "[[ 0.04441286  0.01767773  0.02896742  0.02542425 -0.00261754]\n",
      " [ 0.01310148 -0.03139796 -0.03856922 -0.01637782  0.02233351]\n",
      " [-0.02808004  0.03573376  0.03239204  0.00954127 -0.04970373]\n",
      " [-0.02527453  0.00060741 -0.01384113 -0.04551616  0.04721661]\n",
      " [ 0.03283885 -0.0085416   0.0101666  -0.01604132  0.00731027]]\n",
      "\n",
      "book: 1 link: 0\n",
      "New book embedding:\n",
      "[[ 0.04441286  0.01767773  0.02896742  0.02542425 -0.00261754]\n",
      " [ 0.01262192 -0.03221312 -0.03942732 -0.01727117  0.02231541]\n",
      " [-0.02808004  0.03573376  0.03239204  0.00954127 -0.04970373]\n",
      " [-0.02527453  0.00060741 -0.01384113 -0.04551616  0.04721661]\n",
      " [ 0.03283885 -0.0085416   0.0101666  -0.01604132  0.00731027]]\n",
      "\n",
      "book: 2 link: 0\n",
      "New book embedding:\n",
      "[[ 0.04441286  0.01767773  0.02896742  0.02542425 -0.00261754]\n",
      " [ 0.01262192 -0.03221312 -0.03942732 -0.01727117  0.02231541]\n",
      " [-0.02855744  0.03490083  0.03151303  0.00863565 -0.04971302]\n",
      " [-0.02527453  0.00060741 -0.01384113 -0.04551616  0.04721661]\n",
      " [ 0.03283885 -0.0085416   0.0101666  -0.01604132  0.00731027]]\n",
      "\n",
      "book: 3 link: 2\n",
      "New book embedding:\n",
      "[[ 0.04441286  0.01767773  0.02896742  0.02542425 -0.00261754]\n",
      " [ 0.01262192 -0.03221312 -0.03942732 -0.01727117  0.02231541]\n",
      " [-0.02855744  0.03490083  0.03151303  0.00863565 -0.04971302]\n",
      " [-0.02507836  0.00098496 -0.01448674 -0.04581713  0.048191  ]\n",
      " [ 0.03283885 -0.0085416   0.0101666  -0.01604132  0.00731027]]\n",
      "\n",
      "book: 4 link: 1\n",
      "New book embedding:\n",
      "[[ 0.04441286  0.01767773  0.02896742  0.02542425 -0.00261754]\n",
      " [ 0.01262192 -0.03221312 -0.03942732 -0.01727117  0.02231541]\n",
      " [-0.02855744  0.03490083  0.03151303  0.00863565 -0.04971302]\n",
      " [-0.02507836  0.00098496 -0.01448674 -0.04581713  0.048191  ]\n",
      " [ 0.03204353 -0.00859745  0.0096146  -0.01618481  0.00709641]]\n",
      "\n",
      "book: 0 link: 0\n",
      "New book embedding:\n",
      "[[ 0.04392323  0.01685756  0.02809974  0.02452071 -0.00264686]\n",
      " [ 0.01262192 -0.03221312 -0.03942732 -0.01727117  0.02231541]\n",
      " [-0.02855744  0.03490083  0.03151303  0.00863565 -0.04971302]\n",
      " [-0.02507836  0.00098496 -0.01448674 -0.04581713  0.048191  ]\n",
      " [ 0.03204353 -0.00859745  0.0096146  -0.01618481  0.00709641]]\n",
      "\n",
      "book: 1 link: 2\n",
      "New book embedding:\n",
      "[[ 0.04392323  0.01685756  0.02809974  0.02452071 -0.00264686]\n",
      " [ 0.01280831 -0.03183479 -0.04007936 -0.01759068  0.02330997]\n",
      " [-0.02855744  0.03490083  0.03151303  0.00863565 -0.04971302]\n",
      " [-0.02507836  0.00098496 -0.01448674 -0.04581713  0.048191  ]\n",
      " [ 0.03204353 -0.00859745  0.0096146  -0.01618481  0.00709641]]\n",
      "\n",
      "book: 2 link: 2\n",
      "New book embedding:\n",
      "[[ 0.04392323  0.01685756  0.02809974  0.02452071 -0.00264686]\n",
      " [ 0.01280831 -0.03183479 -0.04007936 -0.01759068  0.02330997]\n",
      " [-0.02836489  0.0352685   0.03084134  0.00830734 -0.04870363]\n",
      " [-0.02507836  0.00098496 -0.01448674 -0.04581713  0.048191  ]\n",
      " [ 0.03204353 -0.00859745  0.0096146  -0.01618481  0.00709641]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in pairs:\n",
    "\n",
    "    # the first value in a pair is an integer representing a book\n",
    "    book_idx = p[0]\n",
    "    # the second value in a pair is an integer representing a link\n",
    "    link_idx = p[1]\n",
    "    print(\"book:\", book_idx, \"link:\", link_idx)\n",
    "\n",
    "    # since we are looping only through positive examples, the true label is 1 for each of them\n",
    "    # The default learning rate for SGD in Keras is 0.01\n",
    "    book_embedding[book_idx] = update_book_embedding(book_embedding[book_idx], link_embedding[link_idx], 1, 0.01)\n",
    "    link_embedding[link_idx] = update_link_embedding(book_embedding[book_idx], link_embedding[link_idx], 1, 0.01)\n",
    "\n",
    "    print(\"New book embedding:\")\n",
    "    print(book_embedding)\n",
    "    #print(\"new link embedding:\")\n",
    "    #print(link_embedding)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compare Keras' book embedding matrix after one training example to ours!\n",
    "<br>\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Keras} & = \n",
    "    \\begin{bmatrix}\n",
    "        \\mathbf{0.04441287} & \\mathbf{0.01767774} & \\mathbf{0.02896742} & \\mathbf{0.02542426} & \\mathbf{-0.00261754}\\\\\n",
    "        0.01310148 & -0.03139796 & -0.03856922 & -0.01637782 & 0.02233351 \\\\\n",
    "        -0.02808004 & 0.03573376 & 0.03239204 & 0.00954127 & -0.04970373 \\\\\n",
    "        -0.02527453 & 0.00060741 & -0.01384113 & -0.04551616 & 0.04721661\\\\\n",
    "        0.03283885 & -0.0085416 &  0.0101666 & -0.01604132 & 0.00731027\n",
    "    \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\text{Ours} & =\n",
    "    \\begin{bmatrix}\n",
    "        \\mathbf{0.04441286} & \\mathbf{0.01767773} & \\mathbf{0.02896742} & \\mathbf{0.02542425} & \\mathbf{-0.00261754}\\\\\n",
    "        0.01310148 & -0.03139796 &-0.03856922 &-0.01637782 & 0.02233351\\\\\n",
    "        -0.02808004 & 0.03573376 & 0.03239204 & 0.00954127& -0.04970373\\\\\n",
    "        -0.02527453 & 0.00060741 &-0.01384113 & -0.04551616 & 0.04721661\\\\\n",
    "        0.03283885 &-0.0085416 &  0.0101666 & -0.01604132 & 0.00731027\\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "## We've matched Keras down to 7 decimal places!!!\n",
    "Close enough for me to feel like we've got a good handle on how these embeddings are actually learned. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
